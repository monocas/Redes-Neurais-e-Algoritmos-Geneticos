{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando uma rede neural usando pytorch\n",
    "========================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O notebook anterior foi o *season finale* da construção da rede neural usando Python puro. Este notebook é o epílogo.\n",
    "\n",
    "Aqui veremos como podemos construir e treinar uma rede neural usando `pytorch`, um módulo especializado em redes neurais artificiais.\n",
    "\n",
    "Motivos para usarmos o `pytorch`:\n",
    "\n",
    "1.  A `MLP` que criamos em Python puro não é otimizada. Tente treinar uma rede com uns 100 neurônios em uma camada e verá como ela é bem lenta! `pytorch` é escrito por diversos programadores que além de primar pelas contas estarem corretas, eles fazem de tudo para que elas ocorram de forma eficiente.\n",
    "\n",
    "2.  `pytorch` já tem praticamente tudo que precisamos implementado! Quer calcular o gradiente local da função arco tangente? Ele já tem! Quer usar a função de ativação ReLU? Ele já tem!\n",
    "\n",
    "3.  `pytorch` tem suporte a treinar uma rede neural usando processamento gráfico (GPU). Isso acelera muito o treino de redes neurais artificiais complexas. Temos GPUs para usarmos no HPC da Ilum!\n",
    "\n",
    "4.  `pytorch` é usado tanto na academia quanto no mundo corporativo. Junto com `tensorflow` são os dois módulos estado da arte para o projeto e treino de redes neurais.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinar uma rede neural artificial tipo Multilayer Perceptron usando `pytorch`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venv\\ilumpy\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código e discussão\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos treinar nossa rede neural com nosso velho amigo, o dataset de diamantes! Como todo processo de treino de aprendizado de máquina, precisamos reservar um conjunto de dados para treino e outro para teste.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAMANHO_TESTE = 0.1\n",
    "SEMENTE_ALEATORIA = 61455\n",
    "DATASET_NAME = \"diamonds\"\n",
    "FEATURES = [\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"]\n",
    "TARGET = [\"price\"]\n",
    "\n",
    "df = sns.load_dataset(DATASET_NAME)\n",
    "\n",
    "indices = df.index\n",
    "indices_treino, indices_teste = train_test_split(\n",
    "    indices, test_size=TAMANHO_TESTE, random_state=SEMENTE_ALEATORIA\n",
    ")\n",
    "\n",
    "df_treino = df.loc[indices_treino]\n",
    "df_teste = df.loc[indices_teste]\n",
    "\n",
    "X_treino = df_treino.reindex(FEATURES, axis=1)\n",
    "y_treino = df_treino.reindex(TARGET, axis=1)\n",
    "X_teste = df_teste.reindex(FEATURES, axis=1)\n",
    "y_teste = df_teste.reindex(TARGET, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes neurais costumam se dar bem quando os dados estão entre $[0, 1]$ ou entre $[-1, 1]$. Redes neurais costumam não se dar bem quando os dados estão em escalas muito diferentes. Por conta disso, normalizar os dados antes de prosseguir é recomendado! Vamos usar o `MinMaxScaler` do `scikit-learn` que aprendemos semestre passado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizador_x = MinMaxScaler()\n",
    "normalizador_y = MinMaxScaler()\n",
    "\n",
    "normalizador_x.fit(X_treino)\n",
    "normalizador_y.fit(y_treino)\n",
    "\n",
    "X_treino = normalizador_x.transform(X_treino)\n",
    "y_treino = normalizador_y.transform(y_treino)\n",
    "X_teste = normalizador_x.transform(X_teste)\n",
    "y_teste = normalizador_y.transform(y_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na nossa rede neural feita em Python puro, a base de tudo era a classe `Valor`. Para o `pytorch`, a base de tudo são os tensores! Tensores são objetos que armazenam dados de forma similar aos arrays de `numpy`, porém registram todas as operações realizadas para o cálculo do gradiente local (assim como nós fizemos com a classe `Valor`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treino = torch.tensor(X_treino, dtype=torch.float32)\n",
    "y_treino = torch.tensor(y_treino, dtype=torch.float32)\n",
    "X_teste = torch.tensor(X_teste, dtype=torch.float32)\n",
    "y_teste = torch.tensor(y_teste, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos checar os dados para ver como estão.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0686, 0.5611, 0.2885, 0.4777, 0.0876, 0.1022],\n",
      "        [0.0624, 0.5250, 0.2500, 0.4730, 0.0871, 0.0994],\n",
      "        [0.0478, 0.4917, 0.2692, 0.4572, 0.0829, 0.0934],\n",
      "        ...,\n",
      "        [0.1185, 0.5333, 0.2692, 0.5410, 0.0995, 0.1142],\n",
      "        [0.0166, 0.5139, 0.2885, 0.3929, 0.0708, 0.0811],\n",
      "        [0.0333, 0.5361, 0.2692, 0.4209, 0.0776, 0.0890]])\n",
      "\n",
      "tensor([[0.0676],\n",
      "        [0.0586],\n",
      "        [0.0434],\n",
      "        ...,\n",
      "        [0.1527],\n",
      "        [0.0172],\n",
      "        [0.0216]])\n"
     ]
    }
   ],
   "source": [
    "print(X_treino)\n",
    "print()\n",
    "print(y_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a rede neural\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar uma rede neural usando `pytorch` é necessário criar uma classe. Observe aqui que a classe criada é baseada na classe `nn.Module` do `pytorch`. Classes podem ser baseadas em outras classes! Trata-se da característica de herança das classes.\n",
    "\n",
    "Observe que definimos as camadas da rede neural dentro de um objeto `nn.Sequential`.\n",
    "\n",
    "Observe também o método `forward`. Este funciona de forma similar ao dunder `__call__` que vimos anteriormente. O `pytorch` requer que usemos o `forward` e não o `__call__`, então aceitamos e seguimos em frente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_dados_entrada, neuronios_c1, neuronios_c2, num_targets\n",
    "    ):\n",
    "        # Temos que inicializar a classe mãe\n",
    "        super().__init__()\n",
    "\n",
    "        # Definindo as camadas da rede\n",
    "        self.camadas = nn.Sequential(\n",
    "                    nn.Linear(num_dados_entrada, neuronios_c1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(neuronios_c1, neuronios_c2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(neuronios_c2, num_targets),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Esse é o método que executa a rede do pytorch.\"\"\"\n",
    "        x = self.camadas(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos criar a nossa rede neural!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DADOS_DE_ENTRADA = X_treino.shape[1]\n",
    "NUM_DADOS_DE_SAIDA = y_treino.shape[1]\n",
    "NEURONIOS_C1 = 50\n",
    "NEURONIOS_C2 = 20\n",
    "\n",
    "minha_MLP = MLP(NUM_DADOS_DE_ENTRADA, NEURONIOS_C1, NEURONIOS_C2, NUM_DADOS_DE_SAIDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E podemos checar os parâmetros internos dela!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0140, -0.0283,  0.2253,  0.1262, -0.2799,  0.2314],\n",
      "        [-0.0676,  0.1150, -0.3239,  0.3064, -0.0710, -0.1175],\n",
      "        [ 0.3124,  0.1934, -0.0649,  0.1261,  0.1928, -0.1205],\n",
      "        [ 0.0602,  0.0562,  0.1754, -0.2576,  0.0349,  0.3436],\n",
      "        [-0.0208, -0.0060,  0.2893,  0.2180,  0.0443, -0.0949],\n",
      "        [ 0.3220,  0.0798,  0.1410, -0.0558, -0.1988, -0.2931],\n",
      "        [-0.1444, -0.1000,  0.3891, -0.3483,  0.0777,  0.4036],\n",
      "        [-0.2996,  0.0044,  0.4016, -0.1807, -0.2669,  0.3132],\n",
      "        [-0.1419, -0.1520, -0.0053,  0.2540, -0.3391, -0.0097],\n",
      "        [-0.2506,  0.0202,  0.1277, -0.0533,  0.1182, -0.1728],\n",
      "        [ 0.0910, -0.0980,  0.1116, -0.0241, -0.0639, -0.0730],\n",
      "        [-0.0205, -0.1600,  0.3279,  0.2244, -0.2857, -0.0417],\n",
      "        [-0.3289,  0.0779,  0.2220, -0.1697,  0.3427, -0.1739],\n",
      "        [ 0.1409,  0.1407,  0.1561,  0.3098,  0.3495, -0.4021],\n",
      "        [ 0.3426,  0.1388,  0.1630, -0.3610,  0.1742,  0.2506],\n",
      "        [ 0.3233, -0.4059,  0.2418, -0.0516, -0.0357, -0.0533],\n",
      "        [-0.2792, -0.1236, -0.3149, -0.1072, -0.2473, -0.3544],\n",
      "        [-0.2441, -0.2716, -0.1229,  0.1308, -0.1595, -0.0585],\n",
      "        [-0.3830, -0.2292,  0.0954,  0.3656,  0.1559,  0.3677],\n",
      "        [ 0.3663,  0.0532,  0.1831,  0.2006, -0.1888, -0.1316],\n",
      "        [ 0.0308, -0.3498,  0.0142, -0.0823,  0.0618, -0.4008],\n",
      "        [-0.1798,  0.1039,  0.3176,  0.3481, -0.2214,  0.3418],\n",
      "        [-0.2565,  0.3279, -0.3394,  0.1432,  0.2407,  0.3090],\n",
      "        [ 0.1683,  0.2852, -0.3463, -0.1026,  0.1563, -0.2162],\n",
      "        [ 0.0585, -0.1164, -0.1946, -0.2912,  0.0078,  0.3507],\n",
      "        [-0.0116,  0.0090,  0.1826,  0.0142,  0.0918, -0.2416],\n",
      "        [-0.2910,  0.0330, -0.3826,  0.3320,  0.1069, -0.2249],\n",
      "        [ 0.3554,  0.1800, -0.0623,  0.2381, -0.0250, -0.3597],\n",
      "        [-0.2719, -0.2501, -0.2361, -0.0697, -0.2781, -0.1241],\n",
      "        [-0.0249,  0.2179, -0.1366,  0.3407,  0.3344, -0.1365],\n",
      "        [ 0.3702, -0.1211,  0.0935, -0.3502,  0.0837, -0.0915],\n",
      "        [ 0.2241, -0.1966, -0.3839,  0.1093, -0.0895,  0.1227],\n",
      "        [ 0.0807,  0.3032, -0.0750,  0.0344, -0.3480,  0.2548],\n",
      "        [ 0.2228, -0.3655, -0.0792,  0.0832, -0.2660,  0.1169],\n",
      "        [ 0.1289, -0.4053,  0.1621,  0.1444, -0.1852, -0.0061],\n",
      "        [-0.2774,  0.3085, -0.0353, -0.2017, -0.1190, -0.1651],\n",
      "        [ 0.0190, -0.1479,  0.1434,  0.3261,  0.0976,  0.0437],\n",
      "        [ 0.1885, -0.0777,  0.0656,  0.2160,  0.2787,  0.2460],\n",
      "        [ 0.2752, -0.2191,  0.1722,  0.1290,  0.0380,  0.1555],\n",
      "        [ 0.0765, -0.3121,  0.3163,  0.2147, -0.1048, -0.1133],\n",
      "        [-0.1802,  0.2061,  0.3691,  0.2408,  0.0558,  0.0456],\n",
      "        [-0.1034,  0.1891, -0.2513, -0.2064, -0.3731, -0.2804],\n",
      "        [-0.1060,  0.3461, -0.2068,  0.3337, -0.0954, -0.3046],\n",
      "        [-0.2306,  0.2016, -0.1462, -0.1927,  0.1741, -0.0216],\n",
      "        [ 0.1283, -0.0923, -0.2815, -0.0611,  0.3727,  0.3120],\n",
      "        [ 0.0143,  0.4014, -0.3475,  0.3597, -0.0426, -0.3489],\n",
      "        [ 0.3549,  0.1529,  0.2618,  0.3223, -0.1322,  0.0529],\n",
      "        [ 0.3742,  0.3559, -0.0139, -0.3913, -0.3794, -0.0786],\n",
      "        [ 0.2621, -0.2622,  0.0705,  0.3982,  0.0304,  0.3676],\n",
      "        [-0.1621,  0.2202, -0.3116, -0.0017, -0.0605, -0.3109]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0111, -0.0465,  0.1923,  0.1447,  0.3109,  0.1497,  0.1835, -0.1915,\n",
      "        -0.2436,  0.0501,  0.3081, -0.2707,  0.2355, -0.1926, -0.0631, -0.2964,\n",
      "        -0.0136,  0.1335, -0.3745, -0.1633,  0.2957,  0.0853,  0.2530, -0.2517,\n",
      "        -0.1967,  0.0600,  0.0954, -0.0507, -0.1323, -0.1960,  0.0440, -0.3513,\n",
      "        -0.3335,  0.0939,  0.1396, -0.0235,  0.1495,  0.2566, -0.2965,  0.3169,\n",
      "        -0.2834, -0.2122,  0.4068,  0.3196, -0.1346, -0.2035,  0.2190, -0.3311,\n",
      "        -0.2217,  0.0309], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0943,  0.0291, -0.1125, -0.0513, -0.0858,  0.0614,  0.1201,  0.1113,\n",
      "         -0.1053, -0.0111, -0.1043, -0.1122,  0.0125, -0.0838, -0.0551, -0.0443,\n",
      "          0.0245,  0.0541, -0.0609, -0.1238, -0.0677, -0.0892, -0.1363,  0.0859,\n",
      "         -0.0632,  0.0908, -0.1368,  0.0061,  0.0133, -0.0885,  0.0655,  0.0976,\n",
      "          0.0243,  0.0652, -0.0192, -0.0334, -0.0594,  0.1014, -0.0827,  0.1005,\n",
      "         -0.0396,  0.1199,  0.1406,  0.0664, -0.0674, -0.0029, -0.0629,  0.1079,\n",
      "         -0.1123,  0.1203],\n",
      "        [ 0.0698, -0.0566,  0.0562, -0.0828,  0.0699,  0.1015,  0.0833, -0.1309,\n",
      "          0.0587,  0.0458,  0.0856,  0.0834, -0.0443, -0.0790, -0.1328,  0.0402,\n",
      "         -0.0795, -0.0619, -0.0316,  0.0418,  0.1268, -0.0970,  0.0248,  0.0994,\n",
      "          0.0567,  0.0128, -0.0397,  0.0151, -0.0376, -0.0292,  0.0126,  0.0171,\n",
      "         -0.0548, -0.0478, -0.1113, -0.1219,  0.0999, -0.0427, -0.0673, -0.0825,\n",
      "          0.0206,  0.0175,  0.0738, -0.1279,  0.0757, -0.0113, -0.0907, -0.1091,\n",
      "          0.1203,  0.0878],\n",
      "        [-0.1193,  0.0777,  0.1029, -0.1267,  0.0125,  0.0734, -0.0330, -0.1365,\n",
      "          0.0463, -0.0026,  0.0417,  0.0734,  0.1018, -0.1152, -0.0245, -0.0195,\n",
      "          0.1314,  0.1333, -0.1052,  0.1302,  0.0737, -0.0271,  0.0424,  0.0221,\n",
      "         -0.0495, -0.0302,  0.0653,  0.0418,  0.1176,  0.1208, -0.0861, -0.0334,\n",
      "          0.0079,  0.0066,  0.0055, -0.1051, -0.0335, -0.0619, -0.0447,  0.1160,\n",
      "          0.1333,  0.1363, -0.1021,  0.1276,  0.0862, -0.0726,  0.0421,  0.0352,\n",
      "         -0.0188,  0.1054],\n",
      "        [ 0.0619,  0.1238, -0.0656,  0.0310,  0.0760, -0.0460,  0.1266, -0.0013,\n",
      "         -0.0490,  0.0187, -0.0525,  0.0611,  0.0936,  0.0343, -0.0564, -0.1222,\n",
      "         -0.0184, -0.0595, -0.0150, -0.1038,  0.0473, -0.0177,  0.1351, -0.1161,\n",
      "         -0.1110,  0.0885, -0.1331, -0.1275,  0.0101,  0.1386, -0.0293,  0.0476,\n",
      "         -0.0957, -0.1257, -0.0514,  0.0461,  0.0313, -0.1371, -0.0515,  0.1305,\n",
      "          0.0145,  0.0709,  0.1182,  0.1268,  0.0034,  0.0155,  0.0933,  0.0141,\n",
      "         -0.0925,  0.0479],\n",
      "        [ 0.0828,  0.1198, -0.1349,  0.0847,  0.1201, -0.0646,  0.1165,  0.1389,\n",
      "          0.0180,  0.1288, -0.0531, -0.0296,  0.0170, -0.1081, -0.1053,  0.1238,\n",
      "         -0.0714, -0.0312, -0.0653,  0.0724, -0.0704,  0.0195, -0.0960, -0.0616,\n",
      "         -0.0896, -0.0206, -0.0733, -0.1329,  0.0829, -0.1047, -0.1093, -0.0328,\n",
      "         -0.1224,  0.0703, -0.1254, -0.1059, -0.1272, -0.0769, -0.0141,  0.0202,\n",
      "         -0.0269,  0.0888, -0.0040,  0.0181,  0.1373,  0.0544, -0.0564, -0.0121,\n",
      "         -0.0953,  0.0866],\n",
      "        [ 0.0272, -0.0552,  0.0332, -0.0031,  0.1133,  0.1144, -0.1160, -0.1233,\n",
      "          0.0362,  0.0142, -0.0446,  0.1340,  0.1411, -0.1108, -0.1296,  0.0316,\n",
      "         -0.1244, -0.0353, -0.0960,  0.1049, -0.0433,  0.0725, -0.0949,  0.0232,\n",
      "         -0.1408, -0.0605, -0.0729, -0.0184,  0.0713,  0.0241,  0.0216, -0.0993,\n",
      "         -0.0767,  0.1406, -0.0005, -0.0355, -0.0263,  0.0080,  0.1362, -0.0193,\n",
      "         -0.0732,  0.0815, -0.0793,  0.1337,  0.0423,  0.0016,  0.0248, -0.0851,\n",
      "          0.0662, -0.0028],\n",
      "        [ 0.0760,  0.0927, -0.0013,  0.0079, -0.1176, -0.1013,  0.0624,  0.1366,\n",
      "         -0.1006,  0.0138,  0.0074,  0.0303, -0.1361,  0.1111, -0.0666, -0.0268,\n",
      "         -0.1303,  0.0720,  0.0014,  0.0039, -0.1082,  0.0171,  0.0395, -0.0366,\n",
      "          0.0471, -0.0303, -0.0516,  0.1094, -0.0689,  0.1343,  0.1139,  0.0554,\n",
      "          0.0231,  0.0599, -0.0122, -0.1345,  0.0512,  0.0179,  0.1093, -0.0280,\n",
      "          0.0702, -0.0801,  0.1222,  0.0458,  0.1114, -0.0834, -0.0227, -0.1344,\n",
      "          0.1036, -0.0402],\n",
      "        [-0.0011,  0.0624,  0.1384,  0.0475, -0.1410, -0.1207,  0.0357, -0.1139,\n",
      "         -0.0132,  0.0544,  0.1330,  0.1149,  0.1368,  0.0048, -0.0461, -0.0009,\n",
      "         -0.0497, -0.1210,  0.1366, -0.1384,  0.0104, -0.0297, -0.0316,  0.0545,\n",
      "          0.0049,  0.0007,  0.1246,  0.0474,  0.0678,  0.0389, -0.0680, -0.0927,\n",
      "          0.0418,  0.0093, -0.0517, -0.0076, -0.0647,  0.0935,  0.0955,  0.0730,\n",
      "          0.1082,  0.0711,  0.0340,  0.0290,  0.0047, -0.1087,  0.0680,  0.0936,\n",
      "         -0.0838, -0.0328],\n",
      "        [-0.1315, -0.1078, -0.0919,  0.0504,  0.0132, -0.0905,  0.0508, -0.0831,\n",
      "          0.1243,  0.0027, -0.0418, -0.1049,  0.0956, -0.0238, -0.0018, -0.0701,\n",
      "          0.0748,  0.1060, -0.0815, -0.0701,  0.1169, -0.0435, -0.0224,  0.0858,\n",
      "         -0.0115,  0.1264,  0.0661,  0.0119,  0.0989,  0.0627,  0.0442, -0.1199,\n",
      "          0.0196, -0.0984,  0.0067, -0.0510,  0.0991, -0.0561, -0.1022, -0.0834,\n",
      "          0.0138, -0.0306, -0.0343,  0.0793,  0.0842, -0.0088,  0.0660,  0.0013,\n",
      "         -0.0177,  0.0861],\n",
      "        [ 0.0051,  0.0293,  0.0226, -0.0102, -0.0754,  0.0326,  0.1133,  0.0249,\n",
      "          0.0151,  0.0751, -0.0982, -0.1135,  0.0204, -0.0871,  0.0913,  0.1292,\n",
      "         -0.1288,  0.1314,  0.1257, -0.0180,  0.1216,  0.0819, -0.1356, -0.1196,\n",
      "          0.0279, -0.0132, -0.0807,  0.0500, -0.0310,  0.1275,  0.0678,  0.0999,\n",
      "         -0.0315,  0.0852, -0.0316,  0.0508,  0.0846, -0.1329,  0.1370,  0.0248,\n",
      "         -0.0981,  0.0985, -0.1027,  0.0214,  0.0257,  0.0793,  0.0971,  0.0571,\n",
      "         -0.0012,  0.1059],\n",
      "        [ 0.0107,  0.0443, -0.0120,  0.0793,  0.0109, -0.0034,  0.0612, -0.0152,\n",
      "          0.0638,  0.0188,  0.1139, -0.1372,  0.0568, -0.0287,  0.0483,  0.0110,\n",
      "         -0.0320, -0.1409,  0.0114,  0.0088, -0.1405,  0.0191,  0.0613, -0.1033,\n",
      "          0.1205, -0.1212, -0.0861,  0.1194,  0.1330, -0.1150,  0.1131, -0.0552,\n",
      "         -0.0687,  0.0439,  0.1098,  0.0886, -0.0783, -0.1009, -0.1246, -0.0929,\n",
      "          0.0814, -0.0776, -0.0389,  0.0224, -0.1093, -0.0160,  0.0165,  0.0565,\n",
      "         -0.0195,  0.0822],\n",
      "        [-0.0259, -0.0997,  0.0915, -0.0592, -0.0362,  0.1152, -0.1401, -0.1046,\n",
      "          0.0889, -0.0995, -0.0481,  0.0956, -0.0413, -0.0433,  0.1322,  0.0761,\n",
      "          0.0067,  0.0738, -0.0480, -0.0216,  0.1210,  0.1399,  0.1005,  0.0728,\n",
      "         -0.0834,  0.0425,  0.1323,  0.1309, -0.1302,  0.0704,  0.0350,  0.1275,\n",
      "         -0.0346, -0.1358,  0.1315,  0.0499, -0.0539,  0.1273, -0.0696,  0.0196,\n",
      "         -0.0274, -0.0860, -0.0556, -0.0886, -0.1033,  0.0716, -0.1174, -0.0421,\n",
      "         -0.0612, -0.0623],\n",
      "        [-0.1144, -0.0465,  0.1304, -0.1088, -0.0157, -0.0035, -0.1241, -0.1019,\n",
      "         -0.1372, -0.0864,  0.0839,  0.0388, -0.1043, -0.1332,  0.0722,  0.0518,\n",
      "          0.0094, -0.1274,  0.1126, -0.0198,  0.1149, -0.0156,  0.1078,  0.1303,\n",
      "          0.1021, -0.0752,  0.0742, -0.0874, -0.0043, -0.1284, -0.0970,  0.0309,\n",
      "          0.1288, -0.1009, -0.0856, -0.0822, -0.0089,  0.1334,  0.0004, -0.0239,\n",
      "          0.0728, -0.0293,  0.0005, -0.1403,  0.0351,  0.0508, -0.1063,  0.0556,\n",
      "          0.1398,  0.0404],\n",
      "        [-0.0257, -0.1101,  0.0844, -0.0416, -0.0374, -0.1064, -0.0864,  0.0509,\n",
      "          0.1295, -0.0231, -0.0238, -0.0604,  0.0498, -0.0740, -0.1336,  0.0994,\n",
      "          0.0744, -0.0234, -0.0738, -0.1365, -0.1344,  0.1219, -0.0843,  0.1391,\n",
      "         -0.0515,  0.0534,  0.0950, -0.0479,  0.0904,  0.0059, -0.0742, -0.0986,\n",
      "         -0.0366,  0.1091,  0.0032, -0.0029, -0.0714, -0.0561, -0.1284,  0.0263,\n",
      "         -0.1206,  0.0363, -0.1365, -0.0736,  0.0444, -0.0020, -0.1206,  0.0881,\n",
      "         -0.0450,  0.0835],\n",
      "        [-0.0203, -0.0697, -0.0597,  0.0857,  0.0873, -0.0599,  0.1222, -0.0460,\n",
      "          0.0189,  0.0379, -0.0594, -0.0824,  0.0943,  0.1277, -0.0962, -0.0627,\n",
      "         -0.0431,  0.1183,  0.1278,  0.0464, -0.0028, -0.0915,  0.0517,  0.0290,\n",
      "         -0.0790,  0.0783, -0.0957,  0.0365, -0.1078,  0.0818,  0.0557, -0.1097,\n",
      "         -0.0614, -0.0338,  0.0550, -0.0832,  0.0564, -0.1025, -0.0508, -0.0668,\n",
      "          0.0734,  0.1317, -0.1158, -0.1094, -0.1211,  0.0445, -0.1029,  0.0023,\n",
      "         -0.1413,  0.0357],\n",
      "        [ 0.1398, -0.0049, -0.0118,  0.1399,  0.0729,  0.0572, -0.0353,  0.0151,\n",
      "          0.0561,  0.0721, -0.0063,  0.0892, -0.1282,  0.0468, -0.0690, -0.1108,\n",
      "         -0.0422, -0.0244, -0.0525, -0.0714, -0.0912,  0.0264, -0.0883, -0.0685,\n",
      "          0.1008,  0.0866,  0.0231,  0.0853, -0.0896,  0.1338, -0.0398,  0.1065,\n",
      "          0.1031, -0.0182, -0.1169, -0.1052,  0.0777, -0.0887,  0.0105, -0.0305,\n",
      "          0.0405, -0.1005,  0.1248,  0.0024,  0.0453, -0.1101,  0.1317, -0.0191,\n",
      "          0.1154,  0.0546],\n",
      "        [ 0.0246, -0.0928,  0.0903,  0.1105,  0.0628,  0.1336, -0.0650,  0.1099,\n",
      "          0.0282, -0.1205,  0.0746, -0.0567, -0.0991,  0.0232,  0.0080,  0.0760,\n",
      "         -0.0412, -0.0431,  0.0105,  0.0946, -0.0925,  0.0052, -0.1385,  0.0597,\n",
      "          0.1316,  0.0284,  0.0594, -0.1013, -0.0816,  0.1317,  0.0341,  0.0803,\n",
      "          0.0561, -0.0372,  0.0411,  0.0480, -0.0656,  0.1254,  0.1168,  0.0297,\n",
      "          0.1261, -0.0242,  0.0271,  0.1104, -0.1055, -0.0056, -0.0570, -0.0923,\n",
      "          0.1033,  0.0231],\n",
      "        [-0.0604, -0.1150, -0.0077,  0.0514,  0.1110,  0.1391,  0.1324, -0.0769,\n",
      "          0.1023, -0.1182, -0.0284, -0.0026, -0.1052, -0.0628, -0.0435,  0.0938,\n",
      "          0.0302, -0.0841,  0.1287, -0.1378,  0.1189, -0.0514,  0.0498,  0.1266,\n",
      "         -0.0226, -0.0941, -0.0529, -0.1370,  0.0420,  0.0271, -0.0809,  0.0670,\n",
      "          0.0817,  0.1045, -0.0048, -0.0291, -0.0494,  0.0931, -0.0283,  0.1375,\n",
      "         -0.0225,  0.0436,  0.0449, -0.1408,  0.1308,  0.0808, -0.1080, -0.0199,\n",
      "         -0.0477, -0.0147],\n",
      "        [-0.0625,  0.0064, -0.0695,  0.1130,  0.0994,  0.0027, -0.0426, -0.0419,\n",
      "         -0.0070, -0.0992, -0.0195, -0.1390,  0.0823,  0.0542, -0.0917,  0.0337,\n",
      "          0.0426,  0.1042,  0.0915,  0.1051, -0.1164,  0.0932,  0.0679, -0.1280,\n",
      "          0.1015,  0.0581, -0.1265, -0.1161, -0.0885,  0.0741,  0.0148, -0.0295,\n",
      "          0.0818,  0.0304,  0.0538,  0.0725,  0.0386, -0.0075,  0.0784,  0.0020,\n",
      "          0.0494,  0.1113,  0.1130,  0.1272, -0.1411,  0.0515, -0.0926,  0.0711,\n",
      "          0.0380,  0.0570],\n",
      "        [ 0.0140,  0.0386, -0.1308, -0.1123,  0.0642,  0.0565, -0.0811,  0.1151,\n",
      "         -0.1402,  0.0152, -0.1186,  0.0821, -0.0327, -0.1205,  0.0029, -0.0654,\n",
      "          0.1268, -0.1055,  0.0630, -0.0757, -0.0942, -0.0979, -0.0296, -0.0475,\n",
      "          0.1226, -0.0454,  0.0032, -0.0818,  0.0238, -0.1360, -0.0511,  0.0945,\n",
      "          0.0892,  0.1046, -0.0563,  0.0206,  0.1180, -0.1028,  0.0135,  0.0454,\n",
      "          0.0385, -0.0154, -0.0157, -0.0603,  0.0407, -0.0839,  0.1127, -0.0816,\n",
      "         -0.0402, -0.1230]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0009,  0.1009, -0.0616, -0.0222, -0.0461,  0.1394,  0.0983, -0.0839,\n",
      "        -0.1146, -0.0967, -0.0226,  0.1155,  0.0999, -0.1135, -0.0206, -0.0433,\n",
      "         0.0256, -0.0467,  0.1255,  0.0788], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1454,  0.0807,  0.0209,  0.1385, -0.0281,  0.0370,  0.2056,  0.1908,\n",
      "          0.0559,  0.0092, -0.1636,  0.2209,  0.0399,  0.1072, -0.1382,  0.0357,\n",
      "          0.0366, -0.1959,  0.1581,  0.0753]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1533], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in minha_MLP.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos realizar uma previsão, mas ela provavelmente será bem ruim!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3352],\n",
       "        [0.3332],\n",
       "        [0.3293],\n",
       "        ...,\n",
       "        [0.3386],\n",
       "        [0.3243],\n",
       "        [0.3287]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prev = minha_MLP(X_treino)\n",
    "y_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A função de perda e o otimizador\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `pytorch` já vem com diversas funções de perda já implementadas. A que computa o erro quadrático médio, por exemplo, já está pronta!\n",
    "\n",
    "Fora isso, precisamos definir o nosso otimizador. O otimizador é quem cuida de atualizar os parâmetros da nossa rede. Nós implementamos na nossa rede em Python puro a descida do gradiente. Aqui usaremos um otimizador que é uma modificação da descida do gradiente, chamado `Adam`. Trata-se, simplificadamente, de uma descida do gradiente com taxa de aprendizado individualizada para cada parâmetro e que se altera ao longo do aprendizado. `Adam` é um poderosíssimo otimizador! Não tem porque não usarmos ele neste caso.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXA_DE_APRENDIZADO = 0.001\n",
    "\n",
    "# função perda será o erro quadrático médio\n",
    "fn_perda = nn.MSELoss()\n",
    "\n",
    "# otimizador será o Adam, um tipo de descida do gradiente\n",
    "otimizador = optim.Adam(minha_MLP.parameters(), lr=TAXA_DE_APRENDIZADO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O treino da rede\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avisar o `pytorch` que iremos treinar a rede, vamos invocar o método `train` da nossa rede. Toda rede recém-criada já está no modo treino, mas aqui deixarei explícito para ficar bem claro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (camadas): Sequential(\n",
       "    (0): Linear(in_features=6, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=20, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minha_MLP.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que o ciclo do treino tem as mesmas etapas que vimos no notebook anterior, nada de novo! Aqui basta observar como executamos essas etapas no `pytorch`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0638)\n",
      "1 tensor(0.0606)\n",
      "2 tensor(0.0578)\n",
      "3 tensor(0.0553)\n",
      "4 tensor(0.0531)\n",
      "5 tensor(0.0511)\n",
      "6 tensor(0.0491)\n",
      "7 tensor(0.0474)\n",
      "8 tensor(0.0462)\n",
      "9 tensor(0.0454)\n",
      "10 tensor(0.0447)\n",
      "11 tensor(0.0442)\n",
      "12 tensor(0.0437)\n",
      "13 tensor(0.0434)\n",
      "14 tensor(0.0431)\n",
      "15 tensor(0.0428)\n",
      "16 tensor(0.0426)\n",
      "17 tensor(0.0424)\n",
      "18 tensor(0.0422)\n",
      "19 tensor(0.0421)\n",
      "20 tensor(0.0418)\n",
      "21 tensor(0.0415)\n",
      "22 tensor(0.0412)\n",
      "23 tensor(0.0409)\n",
      "24 tensor(0.0405)\n",
      "25 tensor(0.0400)\n",
      "26 tensor(0.0396)\n",
      "27 tensor(0.0392)\n",
      "28 tensor(0.0387)\n",
      "29 tensor(0.0383)\n",
      "30 tensor(0.0379)\n",
      "31 tensor(0.0375)\n",
      "32 tensor(0.0371)\n",
      "33 tensor(0.0367)\n",
      "34 tensor(0.0364)\n",
      "35 tensor(0.0360)\n",
      "36 tensor(0.0355)\n",
      "37 tensor(0.0351)\n",
      "38 tensor(0.0345)\n",
      "39 tensor(0.0339)\n",
      "40 tensor(0.0333)\n",
      "41 tensor(0.0328)\n",
      "42 tensor(0.0322)\n",
      "43 tensor(0.0316)\n",
      "44 tensor(0.0311)\n",
      "45 tensor(0.0305)\n",
      "46 tensor(0.0298)\n",
      "47 tensor(0.0292)\n",
      "48 tensor(0.0286)\n",
      "49 tensor(0.0279)\n",
      "50 tensor(0.0272)\n",
      "51 tensor(0.0266)\n",
      "52 tensor(0.0259)\n",
      "53 tensor(0.0252)\n",
      "54 tensor(0.0245)\n",
      "55 tensor(0.0239)\n",
      "56 tensor(0.0232)\n",
      "57 tensor(0.0225)\n",
      "58 tensor(0.0218)\n",
      "59 tensor(0.0211)\n",
      "60 tensor(0.0205)\n",
      "61 tensor(0.0198)\n",
      "62 tensor(0.0191)\n",
      "63 tensor(0.0184)\n",
      "64 tensor(0.0178)\n",
      "65 tensor(0.0171)\n",
      "66 tensor(0.0165)\n",
      "67 tensor(0.0159)\n",
      "68 tensor(0.0153)\n",
      "69 tensor(0.0147)\n",
      "70 tensor(0.0142)\n",
      "71 tensor(0.0136)\n",
      "72 tensor(0.0132)\n",
      "73 tensor(0.0127)\n",
      "74 tensor(0.0122)\n",
      "75 tensor(0.0118)\n",
      "76 tensor(0.0114)\n",
      "77 tensor(0.0110)\n",
      "78 tensor(0.0107)\n",
      "79 tensor(0.0103)\n",
      "80 tensor(0.0100)\n",
      "81 tensor(0.0097)\n",
      "82 tensor(0.0095)\n",
      "83 tensor(0.0093)\n",
      "84 tensor(0.0090)\n",
      "85 tensor(0.0088)\n",
      "86 tensor(0.0087)\n",
      "87 tensor(0.0085)\n",
      "88 tensor(0.0084)\n",
      "89 tensor(0.0082)\n",
      "90 tensor(0.0081)\n",
      "91 tensor(0.0080)\n",
      "92 tensor(0.0079)\n",
      "93 tensor(0.0079)\n",
      "94 tensor(0.0078)\n",
      "95 tensor(0.0077)\n",
      "96 tensor(0.0077)\n",
      "97 tensor(0.0076)\n",
      "98 tensor(0.0076)\n",
      "99 tensor(0.0076)\n",
      "100 tensor(0.0075)\n",
      "101 tensor(0.0075)\n",
      "102 tensor(0.0074)\n",
      "103 tensor(0.0074)\n",
      "104 tensor(0.0074)\n",
      "105 tensor(0.0073)\n",
      "106 tensor(0.0073)\n",
      "107 tensor(0.0072)\n",
      "108 tensor(0.0072)\n",
      "109 tensor(0.0071)\n",
      "110 tensor(0.0071)\n",
      "111 tensor(0.0070)\n",
      "112 tensor(0.0070)\n",
      "113 tensor(0.0070)\n",
      "114 tensor(0.0069)\n",
      "115 tensor(0.0069)\n",
      "116 tensor(0.0068)\n",
      "117 tensor(0.0068)\n",
      "118 tensor(0.0068)\n",
      "119 tensor(0.0067)\n",
      "120 tensor(0.0067)\n",
      "121 tensor(0.0067)\n",
      "122 tensor(0.0066)\n",
      "123 tensor(0.0066)\n",
      "124 tensor(0.0065)\n",
      "125 tensor(0.0065)\n",
      "126 tensor(0.0065)\n",
      "127 tensor(0.0064)\n",
      "128 tensor(0.0064)\n",
      "129 tensor(0.0064)\n",
      "130 tensor(0.0063)\n",
      "131 tensor(0.0063)\n",
      "132 tensor(0.0063)\n",
      "133 tensor(0.0062)\n",
      "134 tensor(0.0062)\n",
      "135 tensor(0.0062)\n",
      "136 tensor(0.0062)\n",
      "137 tensor(0.0062)\n",
      "138 tensor(0.0061)\n",
      "139 tensor(0.0061)\n",
      "140 tensor(0.0061)\n",
      "141 tensor(0.0061)\n",
      "142 tensor(0.0061)\n",
      "143 tensor(0.0061)\n",
      "144 tensor(0.0061)\n",
      "145 tensor(0.0061)\n",
      "146 tensor(0.0061)\n",
      "147 tensor(0.0061)\n",
      "148 tensor(0.0060)\n",
      "149 tensor(0.0060)\n",
      "150 tensor(0.0060)\n",
      "151 tensor(0.0060)\n",
      "152 tensor(0.0060)\n",
      "153 tensor(0.0060)\n",
      "154 tensor(0.0060)\n",
      "155 tensor(0.0060)\n",
      "156 tensor(0.0060)\n",
      "157 tensor(0.0060)\n",
      "158 tensor(0.0060)\n",
      "159 tensor(0.0060)\n",
      "160 tensor(0.0060)\n",
      "161 tensor(0.0060)\n",
      "162 tensor(0.0060)\n",
      "163 tensor(0.0060)\n",
      "164 tensor(0.0060)\n",
      "165 tensor(0.0060)\n",
      "166 tensor(0.0059)\n",
      "167 tensor(0.0059)\n",
      "168 tensor(0.0059)\n",
      "169 tensor(0.0059)\n",
      "170 tensor(0.0059)\n",
      "171 tensor(0.0059)\n",
      "172 tensor(0.0059)\n",
      "173 tensor(0.0059)\n",
      "174 tensor(0.0059)\n",
      "175 tensor(0.0059)\n",
      "176 tensor(0.0059)\n",
      "177 tensor(0.0059)\n",
      "178 tensor(0.0059)\n",
      "179 tensor(0.0059)\n",
      "180 tensor(0.0059)\n",
      "181 tensor(0.0059)\n",
      "182 tensor(0.0059)\n",
      "183 tensor(0.0059)\n",
      "184 tensor(0.0059)\n",
      "185 tensor(0.0059)\n",
      "186 tensor(0.0059)\n",
      "187 tensor(0.0059)\n",
      "188 tensor(0.0059)\n",
      "189 tensor(0.0059)\n",
      "190 tensor(0.0059)\n",
      "191 tensor(0.0059)\n",
      "192 tensor(0.0059)\n",
      "193 tensor(0.0059)\n",
      "194 tensor(0.0059)\n",
      "195 tensor(0.0059)\n",
      "196 tensor(0.0059)\n",
      "197 tensor(0.0059)\n",
      "198 tensor(0.0059)\n",
      "199 tensor(0.0059)\n",
      "200 tensor(0.0059)\n",
      "201 tensor(0.0059)\n",
      "202 tensor(0.0059)\n",
      "203 tensor(0.0059)\n",
      "204 tensor(0.0059)\n",
      "205 tensor(0.0059)\n",
      "206 tensor(0.0059)\n",
      "207 tensor(0.0059)\n",
      "208 tensor(0.0059)\n",
      "209 tensor(0.0059)\n",
      "210 tensor(0.0059)\n",
      "211 tensor(0.0059)\n",
      "212 tensor(0.0059)\n",
      "213 tensor(0.0059)\n",
      "214 tensor(0.0059)\n",
      "215 tensor(0.0059)\n",
      "216 tensor(0.0059)\n",
      "217 tensor(0.0059)\n",
      "218 tensor(0.0059)\n",
      "219 tensor(0.0059)\n",
      "220 tensor(0.0059)\n",
      "221 tensor(0.0059)\n",
      "222 tensor(0.0059)\n",
      "223 tensor(0.0059)\n",
      "224 tensor(0.0059)\n",
      "225 tensor(0.0059)\n",
      "226 tensor(0.0059)\n",
      "227 tensor(0.0059)\n",
      "228 tensor(0.0059)\n",
      "229 tensor(0.0059)\n",
      "230 tensor(0.0059)\n",
      "231 tensor(0.0059)\n",
      "232 tensor(0.0059)\n",
      "233 tensor(0.0059)\n",
      "234 tensor(0.0059)\n",
      "235 tensor(0.0059)\n",
      "236 tensor(0.0059)\n",
      "237 tensor(0.0059)\n",
      "238 tensor(0.0059)\n",
      "239 tensor(0.0058)\n",
      "240 tensor(0.0058)\n",
      "241 tensor(0.0058)\n",
      "242 tensor(0.0058)\n",
      "243 tensor(0.0058)\n",
      "244 tensor(0.0058)\n",
      "245 tensor(0.0058)\n",
      "246 tensor(0.0058)\n",
      "247 tensor(0.0058)\n",
      "248 tensor(0.0058)\n",
      "249 tensor(0.0058)\n",
      "250 tensor(0.0058)\n",
      "251 tensor(0.0058)\n",
      "252 tensor(0.0058)\n",
      "253 tensor(0.0058)\n",
      "254 tensor(0.0058)\n",
      "255 tensor(0.0058)\n",
      "256 tensor(0.0058)\n",
      "257 tensor(0.0058)\n",
      "258 tensor(0.0058)\n",
      "259 tensor(0.0058)\n",
      "260 tensor(0.0058)\n",
      "261 tensor(0.0058)\n",
      "262 tensor(0.0058)\n",
      "263 tensor(0.0058)\n",
      "264 tensor(0.0058)\n",
      "265 tensor(0.0058)\n",
      "266 tensor(0.0058)\n",
      "267 tensor(0.0058)\n",
      "268 tensor(0.0058)\n",
      "269 tensor(0.0058)\n",
      "270 tensor(0.0058)\n",
      "271 tensor(0.0058)\n",
      "272 tensor(0.0058)\n",
      "273 tensor(0.0058)\n",
      "274 tensor(0.0058)\n",
      "275 tensor(0.0058)\n",
      "276 tensor(0.0058)\n",
      "277 tensor(0.0058)\n",
      "278 tensor(0.0058)\n",
      "279 tensor(0.0058)\n",
      "280 tensor(0.0058)\n",
      "281 tensor(0.0058)\n",
      "282 tensor(0.0058)\n",
      "283 tensor(0.0058)\n",
      "284 tensor(0.0058)\n",
      "285 tensor(0.0058)\n",
      "286 tensor(0.0058)\n",
      "287 tensor(0.0058)\n",
      "288 tensor(0.0058)\n",
      "289 tensor(0.0058)\n",
      "290 tensor(0.0058)\n",
      "291 tensor(0.0058)\n",
      "292 tensor(0.0058)\n",
      "293 tensor(0.0058)\n",
      "294 tensor(0.0058)\n",
      "295 tensor(0.0058)\n",
      "296 tensor(0.0058)\n",
      "297 tensor(0.0058)\n",
      "298 tensor(0.0058)\n",
      "299 tensor(0.0058)\n",
      "300 tensor(0.0058)\n",
      "301 tensor(0.0058)\n",
      "302 tensor(0.0058)\n",
      "303 tensor(0.0058)\n",
      "304 tensor(0.0058)\n",
      "305 tensor(0.0058)\n",
      "306 tensor(0.0058)\n",
      "307 tensor(0.0058)\n",
      "308 tensor(0.0058)\n",
      "309 tensor(0.0058)\n",
      "310 tensor(0.0058)\n",
      "311 tensor(0.0058)\n",
      "312 tensor(0.0058)\n",
      "313 tensor(0.0058)\n",
      "314 tensor(0.0058)\n",
      "315 tensor(0.0058)\n",
      "316 tensor(0.0058)\n",
      "317 tensor(0.0058)\n",
      "318 tensor(0.0058)\n",
      "319 tensor(0.0058)\n",
      "320 tensor(0.0058)\n",
      "321 tensor(0.0058)\n",
      "322 tensor(0.0058)\n",
      "323 tensor(0.0058)\n",
      "324 tensor(0.0058)\n",
      "325 tensor(0.0058)\n",
      "326 tensor(0.0058)\n",
      "327 tensor(0.0058)\n",
      "328 tensor(0.0058)\n",
      "329 tensor(0.0058)\n",
      "330 tensor(0.0058)\n",
      "331 tensor(0.0058)\n",
      "332 tensor(0.0058)\n",
      "333 tensor(0.0058)\n",
      "334 tensor(0.0058)\n",
      "335 tensor(0.0058)\n",
      "336 tensor(0.0058)\n",
      "337 tensor(0.0058)\n",
      "338 tensor(0.0058)\n",
      "339 tensor(0.0058)\n",
      "340 tensor(0.0058)\n",
      "341 tensor(0.0058)\n",
      "342 tensor(0.0058)\n",
      "343 tensor(0.0058)\n",
      "344 tensor(0.0058)\n",
      "345 tensor(0.0058)\n",
      "346 tensor(0.0058)\n",
      "347 tensor(0.0058)\n",
      "348 tensor(0.0058)\n",
      "349 tensor(0.0058)\n",
      "350 tensor(0.0058)\n",
      "351 tensor(0.0058)\n",
      "352 tensor(0.0058)\n",
      "353 tensor(0.0058)\n",
      "354 tensor(0.0058)\n",
      "355 tensor(0.0058)\n",
      "356 "
     ]
    }
   ],
   "source": [
    "NUM_EPOCAS = 1000\n",
    "\n",
    "y_true = y_treino\n",
    "\n",
    "for epoca in range(NUM_EPOCAS):\n",
    "    # forward pass\n",
    "    y_pred = minha_MLP(X_treino)\n",
    "\n",
    "    # zero grad\n",
    "    otimizador.zero_grad()\n",
    "\n",
    "    # loss\n",
    "    loss = fn_perda(y_pred, y_true)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # atualiza parâmetros\n",
    "    otimizador.step()\n",
    "\n",
    "    # mostra resultado\n",
    "    print(epoca, loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o treino, podemos checar a performance da nossa rede. Observe a linha que inicia com `with`. Tudo que estiver no bloco do `with torch.no_grad()` é computado normalmente, porém o grafo computacional e os gradientes locais não são computados. Use isso sempre que for realizar contas com tensores fora do treino!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_true = normalizador_y.inverse_transform(y_treino)\n",
    "    y_pred = minha_MLP(X_treino)\n",
    "    y_pred = normalizador_y.inverse_transform(y_pred)\n",
    "\n",
    "for yt, yp in zip(y_true, y_pred):\n",
    "    print(yt, yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O teste da rede\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos supor que você já testou diversas arquiteturas de rede e está pronto para usar os dados de teste. Nesta etapa precisamos indicar para o `pytorch` que o treino acabou e estamos na etapa de avaliação. Fazemos isso rodando o método `eval`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minha_MLP.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos realizar o teste!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_true = normalizador_y.inverse_transform(y_teste)\n",
    "    y_pred = minha_MLP(X_teste)\n",
    "    y_pred = normalizador_y.inverse_transform(y_pred)\n",
    "\n",
    "for yt, yp in zip(y_true, y_pred):\n",
    "    print(yt, yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E, finalmente, computar alguma métrica para medir a performance do nosso modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = mean_squared_error(y_true, y_pred, squared=False)\n",
    "print(f'Loss do teste: {RMSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "<p style=\"text-align: justify\"> Para aqueles que acreditam no final da jornada, era pegadinha! O encerramento oficial do guia de redes neurais se encerra no notebook oito, onde apresentamos um spin off do treinamento de uma rede neural! Esse spin off ocorre porque utilizamos um módulo especializado para redes neurais artificiais denominada de Pytorch.  </p>\n",
    "<p style=\"text-align: justify\"> De modo a demonstrar o funcionamento desse módulo, foi utilizada a biblioteca de diamantes em que o target é preço desses diamantes e os atributos que vão nos ajudar a predizer esse preço são \"carat\", \"depth\", \"table\", \"x\", \"y\" e \"z\".  O dataframe é dividio em modelo e este, para enfim, ser normalizado entre [0,1] ou [-1,1], porque os dados não são tão bem trabalhados quando há escalas diferentes. </p>\n",
    "<p style=\"text-align: justify\"> Uma propriedade interessante do PyTorch é que diferente do Python Puro, ele não utiliza as classes como base e sim uma propriedade denominada de Tensores, sendo responsáveis por representar os dados de entrada, os pesos e os gradientes das redes neurais, podendo ter diferentes dimensões que vão de 0 (tensor escalar) a n (tensor de ordem n). Outra vantagem é que esse módulo possui diversas funções de perdas já implementadas, como a que computa o erro quadrático médio, só que para isso ocorrer devemos definir o nosso otimizador, o responsável pela atualização dos parâmetros da nossa rede, anteriormente o que viamos com o nome de descida do gradiante passa a ser substituído pelo otimizador Adam que essencialmente, diz respeito a uma descida do gradiente com taxa de aprendizado individualizada para cada parâmetro que sofre com mudanças ao longo do aprendizado.</p>\n",
    "<p style=\"text-align: justify\"> O treinamento da rede ocorre pelo método train que excecuta todo o processo anterior de treinamento por poucas linhas de código. Após esse treino, realiza-se o método RMSE para análise do modelo de rede.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Muito obrigada pela aula, Dani :)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilumpy",
   "language": "python",
   "name": "ilumpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
